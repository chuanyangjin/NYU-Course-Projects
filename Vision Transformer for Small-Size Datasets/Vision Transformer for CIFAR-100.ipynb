{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nQgfvQ4tT-ou"
      },
      "source": [
        "## Vision Transformer (ViT)\n",
        "\n",
        "We're going to work with Vision Transformer. We will start to build our own vit model and train it on an image classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nFR6WFmfxw43"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.optim.lr_scheduler import _LRScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGv2wu1MyAPC",
        "outputId": "aeb56e0f-3371-45d5-dffd-75b04694ff0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MmNi93C-4rLb"
      },
      "source": [
        "# VIT Implementation\n",
        "\n",
        "The vision transformer can be seperated into three parts, we will implement each part and combine them in the end.\n",
        "\n",
        "Reference ViT implementation from other libaries: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py and https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UNEtT9SQ4jgx"
      },
      "source": [
        "## PatchEmbedding\n",
        "PatchEmbedding is responsible for dividing the input image into non-overlapping patches and projecting them into a specified embedding dimension. It uses a 2D convolution layer with a kernel size and stride equal to the patch size. The output is a sequence of linear embeddings for each patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rAzsdK5YybDa"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "      super(PatchEmbedding, self).__init__()\n",
        "\n",
        "      assert image_size % patch_size == 0, 'Image size must be divisible by patch size'\n",
        "      \n",
        "      self.patch_size = patch_size\n",
        "      self.embed_dim = embed_dim\n",
        "      self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      batch_size, _, height, width = x.shape\n",
        "      patches = self.proj(x)                                \n",
        "      patches = patches.permute(0, 2, 3, 1).view(batch_size, -1, self.embed_dim)  # (B, embed_dim, H, W) -> (B, H, W, embed_dim) -> (B, H*W, embed_dim)\n",
        "      return patches"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk8v66y6MAS"
      },
      "source": [
        "## MultiHeadSelfAttention\n",
        "\n",
        "This class implements the multi-head self-attention mechanism, which is a key component of the transformer architecture. It consists of multiple attention heads that independently compute scaled dot-product attention on the input embeddings. This allows the model to capture different aspects of the input at different positions. The attention outputs are concatenated and linearly transformed back to the original embedding size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "V1LeAZq-0dQW"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "      super(MultiHeadSelfAttention, self).__init__()\n",
        "      \n",
        "      assert embed_dim % num_heads == 0, 'Embedding dimension must be divisible by the number of heads'\n",
        "        \n",
        "      self.embed_dim = embed_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.head_dim = embed_dim // num_heads\n",
        "        \n",
        "      self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
        "      self.attention_dropout = nn.Dropout(dropout)\n",
        "      self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "      self.proj_drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "      batch_size, seq_length, _ = x.size()\n",
        "        \n",
        "      # Compute query, key, and value matrices\n",
        "      qkv = self.qkv_proj(x)\n",
        "      qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "      qkv = qkv.permute(0, 2, 1, 3)  # Reorder dimensions: (B, num_heads, seq_length, 3 * head_dim)\n",
        "      q, k, v = qkv.chunk(3, dim=-1)\n",
        "      \n",
        "      # Scaled dot-product attention\n",
        "      attn = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "      attn = F.softmax(attn, dim=-1)\n",
        "      attn = self.attention_dropout(attn)\n",
        "      output = attn @ v\n",
        "      \n",
        "      # Combine heads and apply linear projection\n",
        "      output = output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
        "      output = self.proj(output)\n",
        "      output = self.proj_drop(output)\n",
        "      \n",
        "      return output\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NCAURJGJ6jhH"
      },
      "source": [
        "## TransformerBlock\n",
        "This class represents a single transformer layer. It includes a multi-head self-attention sublayer followed by a position-wise feed-forward network (MLP). Each sublayer is surrounded by residual connections.\n",
        "We may also use layer normalization or other type of normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "0rT15Biv6igC"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "      super(TransformerBlock, self).__init__()\n",
        "\n",
        "      self.norm1 = nn.LayerNorm(embed_dim)\n",
        "      self.attention = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "\n",
        "      self.norm2 = nn.LayerNorm(embed_dim)\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(embed_dim, mlp_dim),\n",
        "          nn.GELU(),\n",
        "          nn.Dropout(dropout),\n",
        "          nn.Linear(mlp_dim, embed_dim),\n",
        "          nn.Dropout(dropout),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x + self.attention(self.norm1(x))\n",
        "      x = x + self.mlp(self.norm2(x))\n",
        "      return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rgLfJRUm7EDq"
      },
      "source": [
        "## VisionTransformer\n",
        "This is the main class that assembles the entire Vision Transformer architecture. It starts with the PatchEmbedding layer to create patch embeddings from the input image. A special class token is added to the sequence, and positional embeddings are added to both the patch and class tokens. The sequence of patch embeddings is then passed through multiple TransformerBlock layers. The final output is the logits for all classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tgute9Ab0QP4"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
        "      super(VisionTransformer, self).__init__()\n",
        "\n",
        "      self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
        "      self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "      self.pos_embed = nn.Parameter(torch.randn(1, (image_size // patch_size) ** 2 + 1, embed_dim))\n",
        "\n",
        "      self.transformer_layers = nn.ModuleList([\n",
        "          TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
        "          for _ in range(num_layers)\n",
        "      ])\n",
        "\n",
        "      self.norm = nn.LayerNorm(embed_dim)\n",
        "      self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      batch_size = x.shape[0]\n",
        "\n",
        "      # Patch embedding\n",
        "      x = self.patch_embed(x)\n",
        "\n",
        "      # Add class token\n",
        "      cls_token = self.cls_token.repeat(batch_size, 1, 1)\n",
        "      x = torch.cat([cls_token, x], dim=1)\n",
        "\n",
        "      # Add positional embeddings\n",
        "      x = x + self.pos_embed\n",
        "\n",
        "      # Pass through the transformer layers\n",
        "      for layer in self.transformer_layers:\n",
        "          x = layer(x)\n",
        "\n",
        "      # Class token output\n",
        "      cls_output = x[:, 0]\n",
        "\n",
        "      # Apply layer normalization and final linear layer\n",
        "      logits = self.fc(self.norm(cls_output))\n",
        "\n",
        "      return logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lROdKoO37Uqb"
      },
      "source": [
        "## Let's train the ViT!\n",
        "\n",
        "We will train the vit to do the image classification with cifar100. Free free to change the optimizer and or add other tricks to improve the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "cellView": "form",
        "id": "saOCGCLIx-WI"
      },
      "outputs": [],
      "source": [
        "#@title Augmentation\n",
        "def cutmix_data(x, y):\n",
        "    if 1.0 > 0:\n",
        "        lam = np.random.beta(1.0, 1.0)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    \n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
        "    x_sliced = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "    # adjust lambda to exactly match pixel ratio\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
        "    \n",
        "    return [bbx1, bby1, bbx2, bby2 ], y_a, y_b, lam, x_sliced\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int64(W * cut_rat)\n",
        "    cut_h = np.int64(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def mixup_data(x, y):\n",
        "    lam = np.random.beta(1.0, 1.0)\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    \n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "class RandomErasing(object):\n",
        "    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
        "        self.EPSILON = probability\n",
        "        self.mean = mean\n",
        "        self.sl = sl\n",
        "        self.sh = sh\n",
        "        self.r1 = r1\n",
        "       \n",
        "    def __call__(self, img):\n",
        "\n",
        "        if random.uniform(0, 1) > self.EPSILON:\n",
        "            return img\n",
        "\n",
        "        for _ in range(100):\n",
        "            area = img.size()[1] * img.size()[2]\n",
        "       \n",
        "            target_area = random.uniform(self.sl, self.sh) * area\n",
        "            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n",
        "\n",
        "            h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if w < img.size()[2] and h < img.size()[1]:\n",
        "                x1 = random.randint(0, img.size()[1] - h)\n",
        "                y1 = random.randint(0, img.size()[2] - w)\n",
        "                if img.size()[0] == 3:\n",
        "                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n",
        "                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n",
        "                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n",
        "                else:\n",
        "                    img[0, x1:x1+h, y1:y1+w] = self.mean[1]\n",
        "                return img\n",
        "\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "cellView": "form",
        "id": "GHpj9uz6x_41"
      },
      "outputs": [],
      "source": [
        "#@title Cosine Scheduler\n",
        "class CosineAnnealingWarmupRestarts(_LRScheduler):    \n",
        "    def __init__(self,\n",
        "                 optimizer : torch.optim.Optimizer,\n",
        "                 first_cycle_steps : int,\n",
        "                 cycle_mult : float = 1.,\n",
        "                 max_lr : float = 0.1,\n",
        "                 min_lr : float = 0.001,\n",
        "                 warmup_steps : int = 0,\n",
        "                 gamma : float = 1.,\n",
        "                 last_epoch : int = -1\n",
        "        ):\n",
        "        assert warmup_steps < first_cycle_steps\n",
        "        \n",
        "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
        "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
        "        self.base_max_lr = max_lr # first max learning rate\n",
        "        self.max_lr = max_lr # max learning rate in the current cycle\n",
        "        self.min_lr = min_lr # min learning rate\n",
        "        self.warmup_steps = warmup_steps # warmup step size\n",
        "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
        "        \n",
        "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
        "        self.cycle = 0 # cycle count\n",
        "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
        "        \n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "        \n",
        "        # set learning rate min_lr\n",
        "        self.init_lr()\n",
        "    \n",
        "    def init_lr(self):\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "    \n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "                \n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "cellView": "form",
        "id": "j1Z8bMqHyDVG"
      },
      "outputs": [],
      "source": [
        "#@title Initialization\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cellView": "form",
        "id": "6gPmOODAyFPc"
      },
      "outputs": [],
      "source": [
        "#@title Sampler\n",
        "class RASampler(torch.utils.data.Sampler):\n",
        "    def __init__(self, dataset_len, batch_size, repetitions=1, len_factor=3.0, shuffle=False, drop_last=False):\n",
        "        self.dataset_len = dataset_len\n",
        "        self.batch_size = batch_size\n",
        "        self.repetitions = repetitions\n",
        "        self.len_images = int(dataset_len * len_factor)\n",
        "        self.shuffle = shuffle\n",
        "        self.drop_last = drop_last\n",
        "\n",
        "    def shuffler(self):\n",
        "        if self.shuffle:\n",
        "            new_perm = lambda: iter(np.random.permutation(self.dataset_len))\n",
        "        else:\n",
        "            new_perm = lambda: iter(np.arange(self.dataset_len))\n",
        "        shuffle = new_perm()\n",
        "        while True:\n",
        "            try:\n",
        "                index = next(shuffle)\n",
        "            except StopIteration:\n",
        "                shuffle = new_perm()\n",
        "                index = next(shuffle)\n",
        "            for repetition in range(self.repetitions):\n",
        "                yield index\n",
        "\n",
        "    def __iter__(self):\n",
        "        shuffle = iter(self.shuffler())\n",
        "        seen = 0\n",
        "        batch = []\n",
        "        for _ in range(self.len_images):\n",
        "            index = next(shuffle)\n",
        "            batch.append(index)\n",
        "            if len(batch) == self.batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        if batch and not self.drop_last:\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.drop_last:\n",
        "            return self.len_images // self.batch_size\n",
        "        else:\n",
        "            return (self.len_images + self.batch_size - 1) // self.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "byAC841ix_lb"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "image_size = 32\n",
        "patch_size = 4\n",
        "in_channels = 3\n",
        "embed_dim = 192\n",
        "num_heads = 12\n",
        "mlp_dim = 384\n",
        "num_layers = 8\n",
        "num_classes = 100\n",
        "dropout = 0\n",
        "\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V14TFbM8x4l",
        "outputId": "74482a96-66a4-466e-dfd4-69a96633ccaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ]
        }
      ],
      "source": [
        "model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout).to(device)\n",
        "input_tensor = torch.randn(1, in_channels, image_size, image_size).to(device)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BOp450mdC-D",
        "outputId": "887edb01-409c-4ddf-aba7-bafd266aa6f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load the CIFAR-100 dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5070, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
        "    RandomErasing(probability=0.25, sh=0.4, r1=0.3, mean=(0.5070, 0.4865, 0.4409))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5070, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, num_workers=2, pin_memory=True,\n",
        "    batch_sampler=RASampler(len(trainset), batch_size, 1, 3, shuffle=True, drop_last=True))\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "4s8-X4l-exSg"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)   \n",
        "num_epochs = 100\n",
        "num_steps = int(num_epochs * len(trainloader))\n",
        "warmup_steps = int(10 * len(trainloader))\n",
        "scheduler = CosineAnnealingWarmupRestarts(\n",
        "    optimizer,\n",
        "    first_cycle_steps = num_steps,\n",
        "    cycle_mult = 1.,\n",
        "    max_lr = 0.001,\n",
        "    min_lr = 1e-6,\n",
        "    warmup_steps=warmup_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eOyk345ve5HN",
        "outputId": "02519784-a9b3-483d-b274-dd2da0a08402"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "best_val_acc = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # cutmix and cutout\n",
        "        r = np.random.rand(1)\n",
        "        if r < 0.5:\n",
        "            switching_prob = np.random.rand(1)\n",
        "            # Cutmix\n",
        "            if switching_prob < 0.5:\n",
        "                slicing_idx, y_a, y_b, lam, sliced = cutmix_data(images, labels)\n",
        "                images[:, :, slicing_idx[0]:slicing_idx[2], slicing_idx[1]:slicing_idx[3]] = sliced\n",
        "                outputs = model(images)\n",
        "                \n",
        "                loss =  mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "            # Mixup\n",
        "            else:\n",
        "                images, y_a, y_b, lam = mixup_data(images, labels)\n",
        "                outputs = model(images)\n",
        "                \n",
        "                loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels) \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "  \n",
        "\n",
        "    # Validate the model        \n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    print(f\"Epoch: {epoch + 1}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
